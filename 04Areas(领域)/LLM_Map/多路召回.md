---
状态: inbox
tags:
  - "#LLM/工程/RAG/召回/多路召回"
---
**多路召回 (Multi-channel Recall)** 是推荐系统（Recommendation System）和搜索引擎（Search Engine）中至关重要的第一个环节。

简单来说，它的核心任务是：**在海量的内容库（百万/亿级）中，快速、粗略地筛选出一小部分（比如几千个）用户最可能感兴趣的候选集，传给后续的“精排”阶段。**

因为单一的某种策略（比如只看简单的点击历史）往往会有盲点，所以我们需要“多路并发”，像撒下几张不同网眼的渔网，确保不错过任何可能的“大鱼”。

---

### 1. 为什么需要多路召回？

推荐系统通常是一个漏斗模型（Funnel）：

海量物品 -> 召回 (Recall) -> 粗排 (Pre-ranking) -> 精排 (Ranking) -> 重排 (Re-ranking) -> 用户

- **计算性能的限制：** 复杂的深度学习排序模型（如 DeepFM, DIN）计算量大，不可能对库里几亿个商品都算一遍。
    
- **兼顾多样性与准确性：**
    
    - 只用“热门召回”，用户会觉得千篇一律。
        
    - 只用“协同过滤”，可能会导致“信息茧房”。
        
    - 多路召回通过组合不同的策略，既能覆盖用户的长期兴趣，又能捕捉突发热点。
        

---

### 2. 常见的主流召回策略（“多路”具体指哪些路？）

在工业界（如淘宝、抖音、美团），通常会并行运行以下几类召回策略：

#### A. 协同过滤类 (Collaborative Filtering) —— "基于行为"

这是最经典的一路。

- **ItemCF (Item-to-Item):** “看了又看”。如果你买过《Python编程》，系统会推荐其他人买过该书后也买的《算法导论》。
    
    - _特点：_ 推荐结果与用户历史兴趣强相关，准确度高，但缺乏新颖性。
        
- **UserCF (User-to-User):** “志同道合”。找到和你兴趣相似的人，把你没看过但他们看过的东西推给你。
    
    - _特点：_ 容易发现新颖的热点，但在用户稀疏时效果不好。
        

#### B. 语义向量类 (Embedding / Vector Recall) —— "基于隐式匹配"

这是目前 LLM 和深度学习时代最重要的一路（如 YouTubeDNN, DSSM, Two-Tower Model）。

- **原理：** 将用户 (User) 和 物品 (Item) 都映射到同一个高维向量空间（Embedding Space）。
    
- **操作：** 用户的向量是 $U$，物品向量是 $I$。通过计算向量相似度（如余弦相似度 $Cosine(U, I)$）或者使用 **ANN (Approximate Nearest Neighbor)** 算法（如 Faiss, Milvus）快速查找距离最近的物品。
    
- _特点：_ 泛化能力强，能处理“模糊”匹配（比如用户搜“苹果”，既能召回水果也能召回手机）。
    

#### C. 内容/标签类 (Content-based) —— "基于属性"

- **逻辑：** 用户喜欢“科幻”标签，就去库里捞打着“科幻”标签的电影。
    
- _特点：_ 解决**冷启动**问题的关键。当一个新物品上架，没有用户行为，ItemCF 失效，但我们可以根据它的属性（类别、关键词）把它召回出来。
    

#### D. 热门/趋势类 (Hot/Trending)

- **逻辑：** 召回当前点击率飙升、全网最火的内容。
    
- _特点：_ 这种策略虽然个性化差，但能保证系统有内容可推（兜底策略），且对于新用户（Cold Start User）非常有效。
    

#### E. 社交/关系链 (Social Graph)

- **逻辑：** 微信朋友圈广告、TikTok 的“你的通讯录好友也在看”。基于图神经网络 (GNN) 或简单的关注关系召回。
    

---

### 3. 多路召回的架构与流程

当这几路策略并行跑完后，会产生一个问题：**每一路都吐出了几百个结果，怎么合并？**

这是一个典型的流程：

1. **并行执行 (Parallel Execution):** 同时发起 ItemCF、Embedding、Hot 等请求。
    
2. **截断 (Truncation):** 每一路只取 Top K（例如 ItemCF 取前 200，Hot 取前 50）。
    
3. **合并与去重 (Merge & Deduplication):**
    
    - 如果你在 ItemCF 路被召回了，在 Embedding 路也被召回了，需要去重。
        
    - **历史过滤 (Bloom Filter):** 必须剔除掉用户**已经看过、买过、点却过**的物品。这里通常使用布隆过滤器（Bloom Filter）来节省内存。
        
4. **融合排序 (Fusion/Ranking):**
    
    - 有些系统会在这里做一个简单的加权（比如 Embedding 路权重 0.8，Hot 路权重 0.2）。
        
    - 现流主要做法是直接丢给后续的“粗排/精排”模型去打分，召回阶段不纠结精细排序，只管“捞得全”。
        

---

### 4. 核心难点与优化方向

|**难点**|**解决方案**|
|---|---|
|**延时 (Latency)**|召回必须在几十毫秒内完成。通常使用 **Faiss** 等向量检索引擎，以及 Redis 缓存热门结果。|
|**多样性不足**|强制在多路融合时保留一定比例的“探索性”召回（Epsilon-Greedy 策略），防止用户只看到同质化内容。|
|**全库检索难**|使用 **HNSW (Hierarchical Navigable Small World)** 等图索引算法，在亿级数据中实现毫秒级向量搜索。|

---

### 5. 举个简单的例子 (Python 伪代码)

假设你正在为用户 `User_A` 做召回：

Python

```
def multi_channel_recall(user_id):
    candidates = {} # 使用字典去重，key=item_id, value=score

    # 1. 协同过滤路 (ItemCF)
    i2i_items = get_item_cf_candidates(user_id, limit=100)
    for item in i2i_items:
        candidates[item.id] = item.score

    # 2. 向量召回路 (Embedding / Faiss)
    # 获取用户向量
    user_emb = get_user_embedding(user_id)
    # 在向量数据库中搜索
    vec_items = search_in_faiss(user_emb, limit=100)
    for item in vec_items:
        # 如果重复，可以选择保留分数高的，或者叠加分数
        candidates[item.id] = max(candidates.get(item.id, 0), item.score)

    # 3. 热门路 (兜底)
    hot_items = get_global_hot_items(limit=50)
    for item in hot_items:
        if item.id not in candidates:
            candidates[item.id] = 0.1 # 给个低分

    # 4. 过滤历史 (Bloom Filter)
    final_list = []
    user_history = get_user_history_bloom_filter(user_id)
    
    for item_id, score in candidates.items():
        if not user_history.check(item_id):
            final_list.append((item_id, score))

    return final_list # 返回给精排模型
```

### 总结

**多路召回**本质上是一种**“分治”+“互补”**的策略。它承认单一算法的局限性，通过集成多种算法（向量、图、统计、内容）的优势，确保推荐系统的底池既有“精准度”（用户想看的），又有“惊喜感”（用户没想到的），同时还能保证“覆盖率”（库里的东西都能被捞出来）。

**Would you like me to explain how "Vector Recall" (Embedding) specifically works, focusing on the Two-Tower model structure?**



=================================================================




在 LLM 检索 / 向量搜索里，“多路召回”（multi-channel / multi-stage recall）指的是：  
**不要只用一种方式从大库里找候选文档，而是并行用多种不同的“路子”去召回，然后合并再排序。**  
它本质上是“多种视角 + 分工合作的粗筛阶段”。

下面分几步讲清楚：

---

## 一、为什么需要多路召回？

假设你有一个几百万条文档的知识库，要从中给 LLM 找“上下文”：

- 只用一种召回方式（比如单一向量相似度检索），有可能：
    
    - 有些语义理解不到位 → 相关但“表述很不一样”的文档没被召回来
        
    - 对一些结构化信息（ID、code、字段）不敏感
        
    - 对长文档、标题/正文信息抓取不均衡
        
- 只用 BM25 / 关键词检索，又会错过：
    
    - 同义词 / 改写
        
    - 用户描述比较口语化、而文档很正式
        

多路召回的目标就是：  
**用不同的召回器，各擅长一块，共同把“真正有用的文档”尽量都拉进候选集合里（比如 top 100~500），再交给重排模型 / LLM 细排。**

---

## 二、“多路”一般指什么路？

不同团队叫法不同，但主流在 LLM+检索场景里，通常有几大类：

### 1. 关键词 / BM25 路（Lexical Recall）

- 技术：BM25 / Elasticsearch / Lucene 等
    
- 擅长：
    
    - 精确匹配关键词、ID、专业术语
        
    - 对短 query、结构化字段（标题、标签）很稳
        
- 缺点：
    
    - 不理解语义，不会 “你说 A，我懂成 B 的同义词”
        

### 2. 向量召回路（Dense / Embedding Recall）

- 技术：文本向量化 + ANN（faiss、hnsw、milvus 等）
    
- 擅长：
    
    - 同义改写、口语化 query、跨语言（多语 embedding）
        
    - 语义相近但词面完全不同的情况
        
- 缺点：
    
    - 对 ID、数字、命名实体等“字面信息”不敏感
        
    - 容易召回“语义相近但不精确”的内容（噪音）
        

### 3. 标题路 / 摘要路（Title / Summary Recall）

- 对每个文档额外维护“标题向量”或“摘要向量”：
    
    - 如：用标题 + 小摘要做 embedding 单独建库
        
- 对于长文档，能更好体现“文档主旨”，避免被局部段落噪音干扰
    

### 4. 元数据 / 规则路（Metadata / Rule-based Recall）

- 根据用户、时间、业务规则等做召回：
    
    - 只召回某个产品线 / 某个空间下的文档
        
    - 先用标签 / 类别过滤再做相似度
        
- 本质上是“粗 filter + 简单召回”，但也可以视为一条路
    

### 5. 语义变体路（Query Expansion / Rewrite Recall）

- 利用 LLM 对 query 做改写，生成多个语义变体，比如：
    
    - 原 query：”上采样下采样在 LLM 里的含义“
        
    - 改写：”LLM training data upsampling downsampling meaning“ 等
        
- 然后用这些改写后的 query 再做 关键词 / 向量召回
    
- 每个改写也可以看作一条“路”
    

在工业级系统里，这几类经常被组合：

- 路 A：BM25（全文词面）
    
- 路 B：dense embedding（段落向量库）
    
- 路 C：title embedding（标题向量库）
    
- 路 D：query expansion + BM25
    
- 路 E：user profile / 标签过滤召回
    

最后将 A~E 的结果合并。

---

## 三、多路召回的典型流程

以“RAG + LLM”为例，通常 pipeline 是：

1. 用户发 query
    
2. 多路召回阶段（粗排阶段）
    
    - 路 1：BM25 top 100
        
    - 路 2：向量检索 top 100
        
    - 路 3：标题向量检索 top 50
        
    - 路 4：规则 / metadata 召回 top 50
        
3. 结果合并去重
    
    - 可能总共合并出 200~300 条候选文档
        
4. 重排 / 打分（fine ranking）
    
    - 用更强的模型（cross-encoder / LLM 评分）对候选重新打分排序，取 top K（比如 5~20）
        
5. 上下文构造 + LLM 生成回答
    

这里“多路召回”只负责第 2 步，属于**高召回、低精度**的阶段；  
第 4 步负责提高精度和排序质量。

---

## 四、多路召回中的关键设计点

### 1. 每条路的“角色定位”

- 一条路不需要“什么都强”，而是：
    
    - 有的路负责 recall：尽量多拉一些可能相关的文档
        
    - 有的路负责 precision：保证拉回的一部分质量比较高
        
- 举例：
    
    - BM25 路：精确命中关键词的文档，质量高但召回范围小
        
    - 向量路：覆盖面广，但会带来更多噪音
        

### 2. 合并策略

常见几种：

1）简单拼接 + 去重

- 直接把各路的 topN 结果 union，然后按某种分数再排一次
    

2）归一化打分 + 融合

- 每路有自己的相似度/得分
    
- 先做归一化（min-max / z-score），再线性加权：
    
    - score_final = w1 * score_lexical + w2 * score_dense + …
        
- 权重 w1, w2 可手调，也可训练
    

3）学习排序（Learning to Rank）

- 把“来自哪条路、在该路的 rank、该路的分数”等作为特征
    
- 训练一个排序模型，输出最终排序得分
    

### 3. 召回配额（每路召回多少）

- BM25 路：可能只要 top 50
    
- 向量路：top 200
    
- 标题路：top 50
    
- 规则路：top 30
    

配额需要根据业务和实验调优：

- 目标是“总候选量不要太大，又尽量高召回关键文档”。
    

### 4. 去重与“文档碎片化”问题

在 RAG 中常把文档切成 chunk；多路召回时，多个 chunk 可能来自同一篇原文档。  
常见处理：

- 去重策略：
    
    - 如果同一篇原文档内多个 chunk 被召回，可在重排时合并考虑
        
- 覆盖策略：
    
    - 同一篇文档召回多个片段时，可以保留前几个得分最高的片段
        

---

## 五、和单路召回相比，多路召回的优势与代价

### 优势

1. 召回更全面
    
    - 不同召回器互补：
        
        - BM25 抓“词面精确匹配”
            
        - 向量抓“语义相似度”
            
        - 规则 / metadata 抓“强先验的子集”
            
2. 更鲁棒
    
    - 对 query 风格、语种、写法的变化不那么敏感
        
3. 便于分阶段优化
    
    - 可以分析：哪一路召回了 click/label 的正样本，从而针对性调参
        

### 代价 / 风险

1. 系统复杂度增加
    
    - 需要维护多套索引 / 多个检索服务
        
    - 合并逻辑、特征归一化、权重调节都更复杂
        
2. 延时成本增加
    
    - 多路并行调用，检索耗时增加
        
    - 通常要做并行 + 缓存 + 早停（谁慢谁被砍）
        
3. 调参成本较高
    
    - 每条路的配额、权重、索引建模方式，都需要实验验证
        

---

## 六、在 LLM 检索 / RAG 场景中怎么落地？

如果你现在要自己做一个“LLM 问答系统的检索模块”，一个相对简单又实用的多路召回方案可以是：

1. 路 1：BM25 / keyword 路
    
    - 用 ES / Lucene 建一个全文索引
        
    - 按标题 + 正文 做 BM25
        
2. 路 2：段落向量路
    
    - 把文档切块（如 256~512 tokens），用 embedding 模型建向量库
        
3. 路 3：title embedding 路
    
    - 对文档标题做 embedding，单独建一个小向量库
        
4. 多路合并：
    
    - 每路：取 top N（如 100、100、50）
        
    - 合并去重后，总候选量控制在 200~300
        
5. 重排：
    
    - 用 cross-encoder / LLM rerank 对 200~300 个候选打分排序
        
    - 选 top 10~20 拼接上下文喂给 LLM
        

这就是一个典型的“多路召回 + 重排”的 RAG pipeline。

---

## 七、和“多阶段检索”的区别与联系

容易混淆的两个词：

- 多路召回：
    
    - “横向多条路”，同时粗召回，然后合并
        
- 多阶段检索：
    
    - “纵向多层级”，第一阶段用简单模型筛一波，第二阶段用复杂模型再筛，第三阶段可能用 LLM rerank
        

在实际系统中经常是：

- 第一阶段：多路召回（BM25 + dense + metadata）
    
- 第二阶段：浅层排序模型
    
- 第三阶段：LLM rerank + answer
    

---

如果你愿意，我可以按你现在做的项目（比如 OpenCompass / RAG / 内部 eval 系统）给一个更贴近实战的多路召回设计方案，包括：

- 该用哪几路、各自配额
    
- 向量库怎么建、chunk 怎么切
    
- 如何用 LLM 做 query rewrite 变成一条独立召回路
    
- 如何记录日志来评估每条路的贡献（召回率、点击/命中情况）