好的，作为一名 **AI模型评估专家**，我完全理解这个任务的复杂性和对严谨逻辑的要求。这是一个典型的、旨在深度探测LLM能力的评估框架，特别是其在指令遵循、逻辑一致性和异常处理方面的能力。

为了确保评估的准确性和可复现性，我将你提供的流程梳理成一个标准的作业程序（Standard Operating Procedure, SOP）。这个SOP将作为我执行每一次评估的铁律。

---

### **AI模型评估专家 - 标准作业程序 (SOP)**

**目标：** 对给定‘问题’、‘模型回复’和‘评估问题列表’进行综合分析，输出一份严谨的评估报告。

**核心原则：** 客观、精准、逻辑严密、完全遵循下述流程。

#### **第一阶段：初始化与数据校验**

1. **接收输入：** 确认接收到三个完整的数据实体：
    - '问题' (The Original Prompt)
    - '模型回复' (The Model's Response)
    - '评估问题列表' (The List of Evaluation Questions)
2. **设置状态标志：** 初始化两个全局状态标志，用于追踪关键事件：
    - actionA_triggered = False
    - actionB_triggered = False
    - contradiction_details = None (用于记录actionA中发现的真实矛盾)

#### **第二阶段：逐项分析‘评估问题’ (Iterative Analysis)**

我将遍历'评估问题列表'中的每一个'评估问题'，并按顺序执行以下两个核心步骤。

**步骤 2.1：评估问题的有效性分析 (Validity Analysis)**

- **目标：** 判断'评估问题'是否为原始'问题'的有效子集。
    
- **流程：**
    
    1. **约束比对：** 拆解当前'评估问题'中的所有约束条件。
    2. **溯源检查：** 检查每一个约束条件是否能在原始'问题'中找到明确的出处或逻辑上的直接对应。
    
    - **Case 1: 是有效子集**
        
        - **判定：** 该'评估问题'有效。
        - **行动：** 在报告中详细记录该'评估问题'的每个约束条件在原始'问题'中的具体来源。
    - **Case 2: 不是有效子集**
        
        - **判定：** 该'评估问题'无效。
        - **行动：** 在报告中明确指出是哪个（或哪些）约束条件在原始'问题'中找不到出处。
        - **进入子流程判断：**
            - **if '评估问题'声称原始'问题'存在矛盾：**
                - **验证矛盾：** 我将独立分析原始'问题'，判断这个声称的矛盾是否 **真实存在**。
                    - **if 矛盾真实存在：**
                        - **执行 actionA:**
                            - 设置 actionA_triggered = True。
                            - 记录这个'评估问题'和它指出的真实矛盾内容到 contradiction_details。
                    - **if 矛盾并不存在（评估问题捏造或误判了矛盾）：**
                        - **执行 actionB:**
                            - 设置 actionB_triggered = True。

**步骤 2.2：模型回复的符合度判断 (Compliance Judgment)**

- **目标：** 基于第一阶段的分析结果，判断'模型回复'是否符合每个'评估问题'的要求。
- **这是一个带有“短路机制”的判断流程，其逻辑严格依赖于actionA和actionB的状态。**

#### **第三阶段：生成综合分析报告 (Final Report Synthesis)**

在遍历完所有'评估问题'后，我将根据记录的状态和分析结果，生成最终报告。报告将遵循以下最终裁决逻辑：

**裁决逻辑 1：如果 actionA_triggered 为 True**

- 这表示至少有一个'评估问题'正确地指出了原始'问题'中的一个真实矛盾。这是对模型 **最高难度** 的考验。
- **核心检查点：** '模型回复'是否也指出了这个被contradiction_details记录的真实矛盾？
    - **if '模型回复' 未能指出该矛盾：**
        - **最终结论：** 模型未能通过核心考验。
        - **报告格式：**
            - 对于那个正确指出矛盾的'评估问题'，其结果标记为 **不符合**。
            - 对于**所有其他**的'评估问题'，无论其有效性如何，结果全部标记为 **不确定**。（因为模型在关键的第一步就失败了，后续的评估失去了意义）。
        - **任务结束，输出报告。**
    - **if '模型回复' 成功指出了该矛盾：**
        - _(这是一个隐含逻辑，基于专家评估原则)_ 模型通过了核心考验。现在可以继续评估它在其他方面的表现。
        - 对该'评估问题'标记为 **符合**。
        - 然后，继续按照**裁决逻辑3**的流程处理剩余的'评估问题'。

**裁决逻辑 2：如果 actionB_triggered 为 True (且 actionA_triggered 为 False)**

- 这表示至少有一个'评估问题'错误地声称原始'问题'存在矛盾。
- **最终结论：** 评估流程中存在无效的评估项，需要区别对待。
- **报告格式：**
    - 对于所有触发了actionB的'评估问题'（即捏造矛盾的评估问题），其结果标记为 **不确定**。（因为评估问题本身有误，无法基于它进行有效判断）。
    - 然后，继续按照**裁决逻辑3**的流程正常处理**剩余**的'评估问题'。
    - **任务结束，输出报告。**

**裁决逻辑 3：默认裁决流程 (如果 actionA 和 actionB 均未被触发)**

- 这是最标准的情况，所有'评估问题'要么是有效子集，要么是无效但未声称矛盾。
- **报告格式：**
    - 逐个判断'模型回复'是否符合每一个**被判定为有效**的'评估问题'的要求。
        - 若符合，标记为 **符合**。
        - 若不符合，标记为 **不符合**。
    - 对于所有在步骤2.1中被判定为**无效**的'评估问题'（且未触发A或B），其结果标记为 **不确定**。
    - **任务结束，输出报告。**

**输出报告 4：生成并输出JSON格式的最终报告**

- 在遍历并分析完所有'评估问题'后，你的 唯一任务 是构造并输出一个JSON对象。禁止输出任何JSON格式之外的额外文本、解释或注释。
- 最终输出必须严格遵循以下JSON结构：

```json
{
  "evaluationReport": {
    "taskId": "请在此处填写任务ID或留空",
    "evaluationTimestamp": "请在此处填写评估时的时间戳",
    "modelEvaluated": "请在此处填写被评估模型的名称",
    "evaluationResults": [
      {
        "evaluationItem": "这里是评估问题1的原文",
        "conclusion": "符合/不符合/不确定",
        "remarks": "这里是对评估问题1的详细分析和备注"
      },
      {
        "evaluationItem": "这里是评估问题2的原文",
        "conclusion": "符合/不符合/不确定",
        "remarks": "这里是对评估问题2的详细分析和备注"
      }
    ],
    "summary": {
      "overallConclusion": "符合/部分符合/不符合",
      "notes": "这里是对整体评估结果的总结性说明。"
    }
  }
}
```


---

### **总结**

这个SOP将我的评估工作流程化、标准化，确保每一步都有据可循。它将逻辑判断分为三个层次：

1. **评估问题自身的有效性** (Validity)。
2. **是否触发特殊逻辑** (Contradiction Handling)。
3. **模型回复对有效指令的遵循度** (Compliance)。

我已经将这套流程内化。现在，请提供'问题'、'模型回复'和'评估问题列表'，我将立刻开始执行评估。